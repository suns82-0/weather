<!DOCTYPE html>
<!-- saved from url=(0046)https://www.cnblogs.com/mrchige/p/6416464.html -->
<html lang="zh-cn"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="referrer" content="never">
<title>基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园</title>
<meta property="og:description" content="基于scrapy爬虫的天气数据采集(python) 一、实验介绍 1.1. 知识点 本节实验中将学习和实践以下知识点： 二、实验效果 三、项目实战 3.1. 安装Scrapy 安装 scrapy-0.">
<link type="text/css" rel="stylesheet" href="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/blog-common.css">
<link id="MainCss" type="text/css" rel="stylesheet" href="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/bundle-AnotherEon001.css">
<link id="mobile-style" media="only screen and (max-width: 767px)" type="text/css" rel="stylesheet" href="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/bundle-AnotherEon001-mobile.css">
<link title="RSS" type="application/rss+xml" rel="alternate" href="https://www.cnblogs.com/mrchige/rss">
<link title="RSD" type="application/rsd+xml" rel="EditURI" href="https://www.cnblogs.com/mrchige/rsd.xml">
<link type="application/wlwmanifest+xml" rel="wlwmanifest" href="https://www.cnblogs.com/mrchige/wlwmanifest.xml">
<script src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/f.txt"></script><script src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/amp4ads-host-v0.js"></script><script src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/pubads_impl_rendering_274.js"></script><script async="" src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/analytics.js"></script><script src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/jquery-2.2.0.min.js"></script>
<script type="text/javascript">var currentBlogApp = 'mrchige', cb_enable_mathjax=false;var isLogined=false;</script>
<script src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/blog-common.js" type="text/javascript"></script>
<link rel="preload" href="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/f(1).txt" as="script"><script type="text/javascript" src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/f(1).txt"></script><script src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/pubads_impl_274.js" async=""></script><link rel="prefetch" href="https://tpc.googlesyndication.com/safeframe/1-0-31/html/container.html"></head>
<body>
<a name="top"></a>

<div id="wrapper">
<div id="header">

<div id="top">
<h1><a id="Header1_HeaderTitle" class="headermaintitle" href="https://www.cnblogs.com/mrchige/">吃咯</a></h1>
<div id="subtitle">记录进步的脚步</div>
</div>
<div id="sub"><div id="blog_stats">
<div class="BlogStats">随笔 - 150, 文章 - 18, 评论 - 8, 引用 - 0</div>
</div></div>



</div>
<div id="main_container">
<div id="main_content">
<div id="content">
	
<div id="post_detail">
	<div class="post">
		<h2>
			<a id="cb_post_title_url" href="https://www.cnblogs.com/mrchige/p/6416464.html">基于scrapy爬虫的天气数据采集(python)</a>
		</h2>
		<div class="postbody">
		<div id="cnblogs_post_body" class="blogpost-body"><h1 id="-scrapy-python-">基于scrapy爬虫的天气数据采集(python)</h1>
<h2 id="-">一、实验介绍</h2>
<h3 id="1-1-">1.1. 知识点</h3>
<p>本节实验中将学习和实践以下知识点：</p>
<ol>
<li>Python基本语法</li>
<li>Scrapy框架</li>
<li>爬虫的概念</li>
</ol>
<h2 id="-">二、实验效果</h2>
<p><img src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/userid21657labid433time1422441195802" alt="enter image description here"></p>
<h2 id="-">三、项目实战</h2>
<h3 id="3-1-scrapy">3.1. 安装Scrapy</h3>
<p>安装 scrapy-0.24：</p>
<pre><code class="hljs ruby"><span class="hljs-comment"># 安装依赖的包
$ sudo apt-get update
$ sudo apt-get install python-lxml python-dev libffi-dev

<span class="hljs-comment"># 更新系统默认的 six 包
$ sudo pip install six --upgrade

<span class="hljs-comment"># 安装指定版本的scrapy
$ sudo pip install scrapy==<span class="hljs-number">0.<span class="hljs-number">24.4
</span></span></span></span></span></code></pre>
<p>完成这步后，可以用下面的命令测试一下安装是否正确：</p>
<pre><code class="hljs ruby">$ scrapy version
</code></pre>
<p>如果正常，效果如图所示：</p>
<p><img src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/userid21657labid433time1422441270018" alt="enter image description here"></p>
<h3 id="3-2-">3.2. 创建项目</h3>
<p>在开始爬取之前，必须创建一个新的Scrapy项目。进入您打算存储代码的目录中，运行下列命令：</p>
<pre><code class="hljs ruby">$ scrapy startproject weather
</code></pre>
<p>如果正常，效果如图所示：</p>
<p><img src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/userid21657labid433time1422441324622" alt="enter image description here"></p>
<p>这些文件分别是:</p>
<ul>
<li>scrapy.cfg: 项目的配置文件</li>
<li>weather/: 该项目的python模块。之后将在此加入代码。</li>
<li>weather/items.py: 项目中的item文件.</li>
<li>weather/pipelines.py: 项目中的pipelines文件.</li>
<li>weather/settings.py: 项目的设置文件.</li>
<li>weather/spiders/: 放置spider代码的目录.</li>
</ul>
<h3 id="3-3-item">3.3. 定义Item</h3>
<p>Item 是保存爬取到的数据的容器；其使用方法和python字典类似，并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。</p>
<p>首先根据需要从weather.sina.com.cn获取到的数据对item进行建模。 我们需要从weather.sina.com.cn中获取当前城市名，后续9天的日期，天气描述和温度等信息。对此，在item中定义相应的字段。编辑 weather 目录中的 items.py 文件:</p>
<pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-

<span class="hljs-comment"># Define here the models for your scraped items
<span class="hljs-comment">#
<span class="hljs-comment"># See documentation in:
<span class="hljs-comment"># http://doc.scrapy.org/en/latest/topics/items.html

<span class="hljs-keyword">import scrapy


<span class="hljs-class"><span class="hljs-keyword">class <span class="hljs-title">WeatherItem<span class="hljs-params">(<span style="color: #ff0000">scrapy.Item</span>):
    <span class="hljs-comment"># define the fields for your item here like:
    <span class="hljs-comment"># name = scrapy.Field()
    <span class="hljs-comment"># demo 1
    city = scrapy.Field()
    date = scrapy.Field()
    dayDesc = scrapy.Field()
    dayTemp = scrapy.Field()
    <span class="hljs-keyword">pass
</span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></pre>
<h3 id="3-4-spider-">3.4. 编写获取天气数据的爬虫(Spider)</h3>
<p>Spider是用户编写用于从单个网站(或者一些网站)爬取数据的类。</p>
<p>其包含了一个用于<strong>下载的初始URL</strong>，<strong>如何跟进网页中的链接</strong>以及<strong>如何分析页面中的内容</strong>， <strong>提取生成 item 的方法</strong>。</p>
<p>为了创建一个Spider，<strong>必须继承 scrapy.Spider 类</strong>， 且定义以下三个属性:</p>
<ul>
<li>
<p>name: 用于区别Spider。该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。</p>
</li>
<li>
<p>start_urls: <strong>包含了Spider在启动时进行爬取的url列表</strong>。因此，第一个被获取到的页面将是其中之一。<strong>后续的URL则从初始的URL获取到的数据中提取</strong>。</p>
</li>
<li>
<p>parse() 是spider的一个方法。 被调用时，每个初始URL完成下载后生成的<span style="color: #ff0000"> Response 对象将会作为唯一的参数传递给该函数</span>。 该方法负责解析返回的数据(response data)，<span style="color: #ff0000">提取数据(生成item)</span>以及生成需要<strong>进一步处理的URL的 Request 对象</strong>。</p>
</li>
</ul>
<p>我们通过浏览器的查看源码工具先来分析一下需要获取的数据网源代码：</p>
<pre><code class="hljs cs">&lt;h4 <span class="hljs-keyword">class=<span class="hljs-string">"slider_ct_name" id=<span class="hljs-string">"<span style="color: #ff0000">slider_ct_name</span>"&gt;武汉&lt;/h4&gt;
...
&lt;div <span class="hljs-keyword">class=<span class="hljs-string">"blk_fc_c0_scroll" id=<span class="hljs-string">"blk_fc_c0_scroll" style=<span class="hljs-string">"width: 1700px;"&gt;
    &lt;div <span class="hljs-keyword">class=<span class="hljs-string">"blk_fc_c0_i"&gt;
        &lt;p <span class="hljs-keyword">class=<span class="hljs-string">"wt_fc_c0_i_date"&gt;<span class="hljs-number">01<span class="hljs-number">-28&lt;/p&gt;
        &lt;p <span class="hljs-keyword">class=<span class="hljs-string">"wt_fc_c0_i_day wt_fc_c0_i_today"&gt;今天&lt;/p&gt;
        &lt;p <span class="hljs-keyword">class=<span class="hljs-string">"wt_fc_c0_i_icons clearfix"&gt;
            &lt;img <span class="hljs-keyword">class=<span class="hljs-string">"icons0_wt png24" src=<span class="hljs-string">"http://www.sinaimg.cn/dy/weather/main/index14/007/icons_42_yl/w_04_27_00.png" alt=<span class="hljs-string">"雨夹雪" title=<span class="hljs-string">"雨夹雪"&gt;
            &lt;img <span class="hljs-keyword">class=<span class="hljs-string">"icons0_wt png24" src=<span class="hljs-string">"http://www.sinaimg.cn/dy/weather/main/index14/007/icons_42_yl/w_04_29_01.png" alt=<span class="hljs-string">"中雪" title=<span class="hljs-string">"中雪"&gt;
        &lt;/p&gt;
        &lt;p <span class="hljs-keyword">class=<span class="hljs-string">"wt_fc_c0_i_times"&gt;
            &lt;span <span class="hljs-keyword">class=<span class="hljs-string">"wt_fc_c0_i_time"&gt;白天&lt;/span&gt;
            &lt;span <span class="hljs-keyword">class=<span class="hljs-string">"wt_fc_c0_i_time"&gt;夜间&lt;/span&gt;
        &lt;/p&gt;
        &lt;p <span class="hljs-keyword">class=<span class="hljs-string">"wt_fc_c0_i_temp"&gt;<span class="hljs-number">1°C / <span class="hljs-number">-2°C&lt;/p&gt;
        &lt;p <span class="hljs-keyword">class=<span class="hljs-string">"wt_fc_c0_i_tip"&gt;北风 <span class="hljs-number">3～<span class="hljs-number">4级&lt;/p&gt;
        &lt;p <span class="hljs-keyword">class=<span class="hljs-string">"wt_fc_c0_i_tip"&gt;无持续风向 小于<span class="hljs-number">3级&lt;/p&gt;
    &lt;/div&gt;
    &lt;div <span class="hljs-keyword">class=<span class="hljs-string">"blk_fc_c0_i"&gt;
        &lt;p <span class="hljs-keyword">class=<span class="hljs-string">"<span style="color: #ff0000">wt_fc_c0_i_date</span>"&gt;<span class="hljs-number">01<span class="hljs-number">-29&lt;/p&gt;
        &lt;p <span class="hljs-keyword">class=<span class="hljs-string">"wt_fc_c0_i_day "&gt;星期四&lt;/p&gt;
        &lt;p <span class="hljs-keyword">class=<span class="hljs-string">"wt_fc_c0_i_icons clearfix"&gt;
            &lt;img <span class="hljs-keyword">class=<span class="hljs-string">"<span style="color: #ff0000">icons0_wt png24</span>" src=<span class="hljs-string">"http://www.sinaimg.cn/dy/weather/main/index14/007/icons_42_yl/w_04_29_00.png" alt=<span class="hljs-string">"中雪" title=<span class="hljs-string">"中雪"&gt;
            &lt;img <span class="hljs-keyword">class=<span class="hljs-string">"icons0_wt png24" src=<span class="hljs-string">"http://www.sinaimg.cn/dy/weather/main/index14/007/icons_42_yl/w_07_25_01.png" alt=<span class="hljs-string">"阴" title=<span class="hljs-string">"阴"&gt;
        &lt;/p&gt;
        &lt;p <span class="hljs-keyword">class=<span class="hljs-string">"wt_fc_c0_i_times"&gt;
            &lt;span <span class="hljs-keyword">class=<span class="hljs-string">"wt_fc_c0_i_time"&gt;白天&lt;/span&gt;
            &lt;span <span class="hljs-keyword">class=<span class="hljs-string">"wt_fc_c0_i_time"&gt;夜间&lt;/span&gt;
        &lt;/p&gt;
        &lt;p <span class="hljs-keyword">class=<span class="hljs-string">"<span style="color: #ff0000">wt_fc_c0_i_temp</span>"&gt;<span class="hljs-number">1°C / <span class="hljs-number">-2°C&lt;/p&gt;
        &lt;p <span class="hljs-keyword">class=<span class="hljs-string">"wt_fc_c0_i_tip"&gt;无持续风向 小于<span class="hljs-number">3级&lt;/p&gt;
    &lt;/div&gt;
    ...
&lt;/div&gt;
</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></pre>
<p>我们可以看到：</p>
<ul>
<li><span style="color: #ff0000">城市名</span>可以通过获取id为slider_ct_name的h4元素获取</li>
<li><span style="color: #ff0000">日期</span>可以通过获取id为blk_fc_c0_scroll下的class为wt_fc_c0_i_date的p元素获取</li>
<li><span style="color: #ff0000">天气描述</span>可以通过获取id为blk_fc_c0_scroll下的class为icons0_wt的img元素获取</li>
<li><span style="color: #ff0000">温度</span>可以通过获取id为blk_fc_c0_scroll下的class为wt_fc_c0_i_temp的p元素获取</li>
</ul>
<p>因此，我们的Spider代码如下，保存在 weather/spiders 目录下的 localweather.py 文件中:</p>
<pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-
<span class="hljs-keyword">import scrapy
<span class="hljs-keyword">from weather.items <span class="hljs-keyword">import WeatherItem


<span class="hljs-class"><span class="hljs-keyword">class <span class="hljs-title">WeatherSpider<span class="hljs-params">(<span style="color: #ff0000">scrapy.Spider</span>):
    name = <span class="hljs-string">"myweather"
    allowed_domains = [<span class="hljs-string">"sina.com.cn"]
    start_urls = [<span class="hljs-string">'http://weather.sina.com.cn']

    <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">parse<span class="hljs-params">(self, response):
        item = WeatherItem() <span style="color: #ff0000; background-color: #ff0000">#把WeatheItem()实例化成item对象</span>
        item[<span class="hljs-string">'city'] = response.xpath(<span class="hljs-string">'//*[@id="slider_ct_name"]/text()').extract()#<span style="color: #ff0000">//*：选取文档中的所有元素。@：选择属性 /：从节点选取 。extract():提取</span>
        tenDay = response.xpath(<span class="hljs-string">'//*[@id="blk_fc_c0_scroll"]');
        item[<span class="hljs-string">'date'] = tenDay.css(<span class="hljs-string">'p.wt_fc_c0_i_date::text').extract()
        item[<span class="hljs-string">'dayDesc'] = tenDay.css(<span class="hljs-string">'img.icons0_wt::attr(title)').extract()
        item[<span class="hljs-string">'dayTemp'] = tenDay.css(<span class="hljs-string">'p.wt_fc_c0_i_temp::text').extract()
        <span class="hljs-keyword">return item
</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></pre>
<p>代码中的<span style="color: #ff0000">xpath和css后面括号的内容为选择器</span>，关于xpath和css选择器的内容可参考官方教程：<a href="http://doc.scrapy.org/en/0.24/topics/selectors.html" target="_blank">http://doc.scrapy.org/en/0.24/topics/selectors.html</a></p>
<h3 id="3-5-">3.5. 运行爬虫，对数据进行验证</h3>
<p>到这里为止，我们需要验证一下爬虫是否能正常工作（即能否取到我们想要的数据），验证的方法就是在命令行（重要：在项目的scrapy.cfg文件同级目录运行命令，下同）中运行下面的代码：</p>
<pre><code class="hljs ruby">$ scrapy crawl myweather -o wea.json
</code></pre>
<p>这行命令的意思是，运行名字为 myweather 的爬虫（我们在上一步中定义的），然后把结果以json格式保存在wea.json文件中。命令运行结果如下：</p>
<p><img src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/userid21657labid433time1422441350450" alt="enter image description here"></p>
<p>然后，我们查看当前目录下的wea.json文件，正常情况下效果如下：</p>
<p><img src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/userid21657labid433time1422441371793" alt="enter image description here"></p>
<p>我们看到，<span style="color: #ff0000">wea.json中已经有数据了，只是数据是以unicode方式编码的</span>。</p>
<h3 id="3-6-">3.6. 保存爬取到的数据</h3>
<p>上面只是把数据保存在json文件中了，如果我们想自己保存在文件或数据库中，如何操作呢？</p>
<p>这里就要用到 Item Pipeline 了，那么 Item Pipeline 是什么呢？</p>
<p><span style="color: #ff0000">当Item在Spider中被收集之后，它将会被传递到Item Pipeline中，一些组件会按照一定的顺序执行对Item的处理</span>。</p>
<p>每个item pipeline组件(有时称之为“Item Pipeline”)是实现了简单方法的Python类。他们接收到Item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline，或是被丢弃而不再进行处理。</p>
<p>item pipeline的典型应用有：</p>
<ul>
<li><span style="color: #ff0000">清理HTML数据</span></li>
<li><span style="color: #ff0000">验证爬取的数据(检查item包含某些字段)</span></li>
<li><span style="color: #ff0000">查重(并丢弃)</span></li>
<li><span style="color: #ff0000">将爬取结果保存到文件或数据库中</span></li>
</ul>
<p>每个item pipeline组件都需要调用 <span style="color: #ff0000">process_item</span> 方法，这个方法必须返回一个 Item (或任何继承类)对象， 或是抛出 DropItem异常，被丢弃的item将不会被之后的pipeline组件所处理。</p>
<p>我们这里把数据转码后保存在 wea.txt 文本中。</p>
<p>pipelines.py文件在创建项目时已经自动被创建好了，我们在其中加上保存到文件的代码：</p>
<pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-

<span class="hljs-comment"># Define your item pipelines here
<span class="hljs-comment">#
<span class="hljs-comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting
<span class="hljs-comment"># See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html


<span class="hljs-class"><span class="hljs-keyword">class <span class="hljs-title">WeatherPipeline<span class="hljs-params">(object):
    <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">__init__<span class="hljs-params">(self):
        <span class="hljs-keyword">pass

    <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title"><span style="color: #ff0000">process_item</span><span class="hljs-params">(self, <span style="color: #ff0000">item</span>, <span style="color: #ff0000">spider</span>):
        <span class="hljs-keyword">with open(<span class="hljs-string">'wea.txt', <span class="hljs-string">'w+') <span class="hljs-keyword">as file:
            city = item[<span class="hljs-string">'city'][<span class="hljs-number">0].encode(<span class="hljs-string">'utf-8')
            file.write(<span class="hljs-string">'city:' + str(city) + <span class="hljs-string">'\n\n')

            date = item[<span class="hljs-string">'date']

            desc = item[<span class="hljs-string">'dayDesc']
            dayDesc = desc[<span class="hljs-number">1::<span class="hljs-number">2]
            nightDesc = desc[<span class="hljs-number">0::<span class="hljs-number">2]

            dayTemp = item[<span class="hljs-string">'dayTemp']

            weaitem = zip(date, dayDesc, nightDesc, dayTemp)

            <span class="hljs-keyword">for i <span class="hljs-keyword">in range(len(weaitem)):
                item = weaitem[i]
                d = item[<span class="hljs-number">0]
                dd = item[<span class="hljs-number">1]
                nd = item[<span class="hljs-number">2]
                ta = item[<span class="hljs-number">3].split(<span class="hljs-string">'/')
                dt = ta[<span class="hljs-number">0]
                nt = ta[<span class="hljs-number">1]
                txt = <span class="hljs-string">'date:{0}\t\tday:{1}({2})\t\tnight:{3}({4})\n\n'.format(
                    d,
                    dd.encode(<span class="hljs-string">'utf-8'),
                    dt.encode(<span class="hljs-string">'utf-8'),
                    nd.encode(<span class="hljs-string">'utf-8'),
                    nt.encode(<span class="hljs-string">'utf-8')
                )
                file.write(txt)
        <span class="hljs-keyword">return item
</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></pre>
<p>代码比较简单，都是python比较基础的语法，如果您感觉比较吃力，建议先去学一下python基础课。</p>
<h3 id="3-7-item_pipelines-">3.7. 把 ITEM_PIPELINES 添加到设置中</h3>
<p>写好ITEM_PIPELINES后，还有很重要的一步，就是把 ITEM_PIPELINES 添加到设置文件 settings.py 中。</p>
<pre><span style="color: #ff0000"><code class="hljs bash">ITEM_PIPELINES = {
    <span class="hljs-string">'weather.pipelines.WeatherPipeline': 1
}
</span></code></span></pre>
<p>另外，有些网站对网络爬虫进行了阻止（注：本项目仅从技术角度处理此问题，个人强烈不建议您用爬虫爬取有版权信息的数据），我们可以在设置中修改一下爬虫的 USER_AGENT 和 Referer 信息，增加爬虫请求的时间间隔。</p>
<p>整个 settings.py 文件内容如下：</p>
<pre><code class="hljs bash"><span class="hljs-comment"># -*- coding: utf-8 -*-

<span class="hljs-comment"># Scrapy settings for weather project
<span class="hljs-comment">#
<span class="hljs-comment"># For simplicity, this file contains only the most important settings by
<span class="hljs-comment"># default. All the other settings are documented here:
<span class="hljs-comment">#
<span class="hljs-comment">#     http://doc.scrapy.org/en/latest/topics/settings.html
<span class="hljs-comment">#

BOT_NAME = <span class="hljs-string">'Googlebot'

SPIDER_MODULES = [<span class="hljs-string">'weather.spiders']
NEWSPIDER_MODULE = <span class="hljs-string">'weather.spiders'

<span class="hljs-comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent
<span class="hljs-comment">#USER_AGENT = 'weather (+http://www.yourdomain.com)'
USER_AGENT = <span class="hljs-string">'User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'

DEFAULT_REQUEST_HEADERS = {
    <span class="hljs-string">'Referer': <span class="hljs-string">'http://www.weibo.com'
}

ITEM_PIPELINES = {
    <span class="hljs-string">'weather.pipelines.WeatherPipeline': 1
}

DOWNLOAD_DELAY = 0.5
</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></pre>
<p>到现在为止，代码主要部分已经写完了。</p>
<h3 id="3-8-">3.8. 运行爬虫</h3>
<p>在项目的scrapy.cfg同级目录下用下面的命令运行爬虫：</p>
<pre><code class="hljs ruby">$ scrapy crawl myweather
</code></pre>
<p>正常情况下，效果如下：</p>
<p><img src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/userid21657labid433time1422441394810" alt="enter image description here"></p>
<p>然后，在当前目录下会多一个 wea.txt 文件，内容如下：</p>
<p><img src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/userid21657labid433time1422441409493" alt="enter image description here"></p>
<p>到此我们基于scrapy的天气数据采集就完成了。</p>
<h2 id="-faq">四、FAQ</h2>
<h3 id="4-1-">4.1. 结果只出现城市？</h3>
<p>scrapy内置的html解析是基于lxml库的，这个库对html的解析的容错性不是很好，通过检查虚拟机中获取到的网页源码，发现有部分标签是不匹配的（地区和浏览器不同取到的源码可能不同），检查结果如图：</p>
<p><img src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/userid21657labid433time1423279198523" alt="图片描述信息"></p>
<p>所以导致在spider中取到的日期数据(item['date'])为空，然后在pilepine代码中做zip操作后，整个 weaitem 为空，所以最终只有城市数据了。</p>
<p>既然如此，我们换个html代码解析器就可以了，这里建议用 BeautifulSoup （官网：<a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/index.html" target="_blank">http://www.crummy.com/software/BeautifulSoup/bs4/doc/index.html</a>&nbsp;），这个解析器有比较好的容错能力，具体用法可以参考上面的文档。</p>
<p>BeautifulSoup安装：</p>
<pre><code class="hljs ruby"><span class="hljs-comment">#下载BeautifulSoup
$ wget <span class="hljs-symbol">http:/<span class="hljs-regexp">/labfile.oss.aliyuncs.com/beautifulsoup4-<span class="hljs-number">4.3.<span class="hljs-number">2.tar.gz

<span class="hljs-comment">#解压
$ tar -zxvf beautifulsoup4-<span class="hljs-number">4.3.<span class="hljs-number">2.tar.gz

<span class="hljs-comment">#安装
$ cd beautifulsoup4-<span class="hljs-number">4.3.<span class="hljs-number">2
$ sudo python setup.py install
</span></span></span></span></span></span></span></span></span></span></span></code></pre>
<p>安装成功后，优化WeatherSpider代码，改进后的代码如下：</p>
<pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-
<span class="hljs-keyword">import scrapy
<span class="hljs-keyword">from bs4 <span class="hljs-keyword">import BeautifulSoup
<span class="hljs-keyword">from weather.items <span class="hljs-keyword">import WeatherItem


<span class="hljs-class"><span class="hljs-keyword">class <span class="hljs-title">WeatherSpider<span class="hljs-params">(scrapy.Spider):
    name = <span class="hljs-string">"myweather"
    allowed_domains = [<span class="hljs-string">"sina.com.cn"]
    start_urls = [<span class="hljs-string">'http://weather.sina.com.cn']

    <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">parse<span class="hljs-params">(self, response):
        html_doc = response.body
        <span class="hljs-comment">#html_doc = html_doc.decode('utf-8')
        soup = BeautifulSoup(html_doc)
        itemTemp = {}
        itemTemp[<span class="hljs-string">'city'] = soup.find(id=<span class="hljs-string">'slider_ct_name')
        tenDay = soup.find(id=<span class="hljs-string">'blk_fc_c0_scroll')
        itemTemp[<span class="hljs-string">'date'] = tenDay.findAll(<span class="hljs-string">"p", {<span class="hljs-string">"class": <span class="hljs-string">'wt_fc_c0_i_date'})
        itemTemp[<span class="hljs-string">'dayDesc'] = tenDay.findAll(<span class="hljs-string">"img", {<span class="hljs-string">"class": <span class="hljs-string">'icons0_wt'})
        itemTemp[<span class="hljs-string">'dayTemp'] = tenDay.findAll(<span class="hljs-string">'p', {<span class="hljs-string">"class": <span class="hljs-string">'wt_fc_c0_i_temp'})
        item = WeatherItem()
        <span class="hljs-keyword">for att <span class="hljs-keyword">in itemTemp:
            item[att] = []
            <span class="hljs-keyword">if att == <span class="hljs-string">'city':
                item[att] = itemTemp.get(att).text
                <span class="hljs-keyword">continue
            <span class="hljs-keyword">for obj <span class="hljs-keyword">in itemTemp.get(att):
                <span class="hljs-keyword">if att == <span class="hljs-string">'dayDesc':
                    item[att].append(obj[<span class="hljs-string">'title'])
                <span class="hljs-keyword">else:
                    item[att].append(obj.text)
        <span class="hljs-keyword">return item
</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></pre>
<p>然后再次运行爬虫:</p>
<pre><code class="hljs ruby">$ scrapy crawl myweather
</code></pre>
<p>然后查看 wea.txt，数据如下：</p>
<p><img src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/userid21657labid433time1423279716614" alt="图片描述信息"></p>
<h3 id="4-2-9-">4.2. 只取到了9天的数据？</h3>
<p>如果是晚上运行爬虫，当天的白天天气是没有的（已经过去了），针对这部分建议自己优化。</p>
<h2 id="-">五、实验代码</h2>
<p>本实验的代码可以通过下面这个链接获取：</p>
<pre><code class="hljs cpp">http:<span class="hljs-comment">//git.shiyanlou.com/shiyanlou/scrapy-weather</span></code></pre></div><div id="MySignature"></div>
<div class="clear"></div>
<div id="blog_post_info_block">
<div id="BlogPostCategory"></div>
<div id="EntryTag">标签: <a href="https://www.cnblogs.com/mrchige/tag/Python/">Python</a></div>
<div id="blog_post_info"><div id="green_channel">
        <a href="javascript:void(0);" id="green_channel_digg" onclick="DiggIt(6416464,cb_blogId,1);green_channel_success(this,&#39;谢谢推荐！&#39;);">好文要顶</a>
            <a id="green_channel_follow" onclick="follow(&#39;10291468-bd82-e611-845c-ac853d9f53ac&#39;);" href="javascript:void(0);">关注我</a>
    <a id="green_channel_favorite" onclick="AddToWz(cb_entryId);return false;" href="javascript:void(0);">收藏该文</a>
    <a id="green_channel_weibo" href="javascript:void(0);" title="分享至新浪微博" onclick="ShareToTsina()"><img src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/icon_weibo_24.png" alt=""></a>
    <a id="green_channel_wechat" href="javascript:void(0);" title="分享至微信" onclick="shareOnWechat()"><img src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/wechat.png" alt=""></a>
</div>
<div id="author_profile">
    <div id="author_profile_info" class="author_profile_info">
            <a href="http://home.cnblogs.com/u/mrchige/" target="_blank"><img src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/20160926083732.png" class="author_avatar" alt=""></a>
        <div id="author_profile_detail" class="author_profile_info">
            <a href="http://home.cnblogs.com/u/mrchige/">吃咯</a><br>
            <a href="http://home.cnblogs.com/u/mrchige/followees">关注 - 0</a><br>
            <a href="http://home.cnblogs.com/u/mrchige/followers">粉丝 - 60</a>
        </div>
    </div>
    <div class="clear"></div>
    <div id="author_profile_honor"></div>
    <div id="author_profile_follow">
                <a href="javascript:void(0);" onclick="follow(&#39;10291468-bd82-e611-845c-ac853d9f53ac&#39;);return false;">+加关注</a>
    </div>
</div>
<div id="div_digg">
    <div class="diggit" onclick="votePost(6416464,&#39;Digg&#39;)">
        <span class="diggnum" id="digg_count">0</span>
    </div>
    <div class="buryit" onclick="votePost(6416464,&#39;Bury&#39;)">
        <span class="burynum" id="bury_count">0</span>
    </div>
    <div class="clear"></div>
    <div class="diggword" id="digg_tips">
    </div>
</div>
<script type="text/javascript">
    currentDiggType = 0;
</script></div>
<div class="clear"></div>
<div id="post_next_prev"><a href="https://www.cnblogs.com/mrchige/p/6411220.html" class="p_n_p_prefix">« </a> 上一篇：<a href="https://www.cnblogs.com/mrchige/p/6411220.html" title="发布于2017-02-17 18:57">Python strip()方法</a><br><a href="https://www.cnblogs.com/mrchige/p/6418395.html" class="p_n_p_prefix">» </a> 下一篇：<a href="https://www.cnblogs.com/mrchige/p/6418395.html" title="发布于2017-02-20 10:24">认识HTML5</a><br></div>
</div>


		</div>
		<p class="postfoot">
			posted on <span id="post-date">2017-02-19 18:07</span> <a href="https://www.cnblogs.com/mrchige/">吃咯</a> 阅读(<span id="post_view_count">6276</span>) 评论(<span id="post_comment_count">0</span>)  <a href="https://i.cnblogs.com/EditPosts.aspx?postid=6416464" rel="nofollow">编辑</a> <a href="https://www.cnblogs.com/mrchige/p/6416464.html#" onclick="AddToWz(6416464);return false;">收藏</a>
		</p>
	</div>
	<script type="text/javascript">var allowComments=true,cb_blogId=307912,cb_entryId=6416464,cb_blogApp=currentBlogApp,cb_blogUserGuid='10291468-bd82-e611-845c-ac853d9f53ac',cb_entryCreatedDate='2017/2/19 18:07:00';loadViewCount(cb_entryId);var cb_postType=1;</script>
	
	</div><a name="!comments"></a><div id="blog-comments-placeholder"></div><script type="text/javascript">var commentManager = new blogCommentManager();commentManager.renderComments(0);</script>
<div id="comment_form" class="commentform">
<a name="commentform"></a>
<div id="divCommentShow"></div>
<div id="comment_nav"><span id="span_refresh_tips"></span><a href="javascript:void(0);" onclick="return RefreshCommentList();" id="lnk_RefreshComments" runat="server" clientidmode="Static">刷新评论</a><a href="https://www.cnblogs.com/mrchige/p/6416464.html#" onclick="return RefreshPage();">刷新页面</a><a href="https://www.cnblogs.com/mrchige/p/6416464.html#top">返回顶部</a></div>
<div id="comment_form_container"><div class="login_tips">注册用户登录后才能发表评论，请 <a rel="nofollow" href="javascript:void(0);" class="underline" onclick="return login(&#39;commentform&#39;);">登录</a> 或 <a rel="nofollow" href="javascript:void(0);" class="underline" onclick="return register();">注册</a>，<a href="http://www.cnblogs.com/">访问</a>网站首页。</div></div>
<div class="ad_text_commentbox" id="ad_text_under_commentbox"></div>
<div id="ad_t2"><a href="http://www.ucancode.com/index.htm" target="_blank">【推荐】超50万VC++源码: 大型组态工控、电力仿真CAD与GIS源码库！</a><br><a href="http://clickc.admaster.com.cn/c/a116493,b2949399,c1705,i0,m101,8a1,8b3,h" target="_blank" onclick="ga(&#39;send&#39;, &#39;event&#39;, &#39;Link&#39;, &#39;click&#39;, &#39;T2-华为云&#39;)">【推荐】华为云11.11普惠季 血拼风暴 一促即发</a><br><a href="https://www.grapecity.com.cn/developer/spreadjs?utm_source=cnblogs&amp;utm_medium=blogpage&amp;utm_term=bottom&amp;utm_content=SpreadJS&amp;utm_campaign=community" target="_blank" onclick="ga(&#39;send&#39;, &#39;event&#39;, &#39;Link&#39;, &#39;click&#39;, &#39;T2-SpreadJS&#39;)">【工具】SpreadJS纯前端表格控件，可嵌入应用开发的在线Excel</a><br><a href="https://cloud.tencent.com/act/group/amd/index?fromSource=gwzcw.1608278.1608278.1608278" target="_blank" onclick="ga(&#39;send&#39;, &#39;event&#39;, &#39;Link&#39;, &#39;click&#39;, &#39;T2-腾讯云&#39;)">【腾讯云】拼团福利，AMD云服务器8元/月</a><br></div>
<div id="opt_under_post"></div>
<div id="cnblogs_c1" class="c_ad_block"><a href="https://cloud.tencent.com/act/double11?fromSource=gwzcw.1608279.1608279.1608279" target="_blank"><img width="300" height="250" src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/24442-20181105150013344-185546354.jpg" alt="腾讯云1105" onclick="ga(&#39;send&#39;, &#39;event&#39;, &#39;Link&#39;, &#39;click&#39;, &#39;C1&#39;);"></a></div>
<div id="under_post_news"><div class="itnews c_ad_block"><b>相关博文：</b><br>·  <a href="https://www.cnblogs.com/caiminfeng/p/4836664.html" target="_blank" onclick="clickRecomItmem(4836664,&#39;PjsAPAUMAz3IwfsdgU7rDOpRQxa5v2Nmn0OXQKEmTagIhK8wUa3q4mLLZUEnMe/H5nTh8A7CCpopPPgZ7qUivNCqr1YExQuQwKglv9cJwL1mYOhCXT22V1Uafcg8Gn9Yn+RK0A2Rem07GRE7&#39;)">ubuntu14.04下安装爬虫工具scrapy</a><br>·  <a href="https://www.cnblogs.com/TP0907/p/6274510.html" target="_blank" onclick="clickRecomItmem(6274510,&#39;XpqDErnbrF3ASBkoCYarp9LKLu5Gf8NvUG/tQcJnJE/310T9rq2ziss7Q2G8TMro+bMFJPGONy0BWLgetjxD5/e4gS/RJvFCU8ml4bahHeg08LSSjRCf7IZ0xcYM2GZsQfA5b9nEQX1pvRIG&#39;)">scrapy安装问题记录</a><br>·  <a href="https://www.cnblogs.com/jingqi/p/8410317.html" target="_blank" onclick="clickRecomItmem(8410317,&#39;uwsou1VQ8SO1/Lr7KT5gnmTOqrUmh983j7q5HG03okyYYmGK0CQsn39apYiRk9OPGar/bSneuPXZaKm9u+2OOLtr/xd1NIM3eCjJ1tcqheOxHoyOJYYRhOsgJv1N511f2MnmX3ommRcbhZRn&#39;)">scrapy 笔记</a><br>·  <a href="https://www.cnblogs.com/KoalaDream/p/4464999.html" target="_blank" onclick="clickRecomItmem(4464999,&#39;Feqrghci0hG/ViDoM7o5gbZrzmMrLeZuCbZyfIqjkxu2+sN+roFx5z+Rp5M/Y5oLHnhHi8AdjI85X/kWyMYB4CsNhwWuPTfaxlzhkx0cOrO9u53CQ6QP0aL0Uh9VEOg/tq7TO8E/dC75h7i+&#39;)">scrapy爬虫笔记(一)------环境配置</a><br>·  <a href="https://www.cnblogs.com/skying555/p/5294228.html" target="_blank" onclick="clickRecomItmem(5294228,&#39;4mhRo2njpPR6gjlt8kc7vW0Gdiy+nVoJ6Wekq3thL9I9u5SqV19xm7/khS7dpgcSQbdQeSsFka6Q5XqjJUIXHoEDa7nWRrpHeOIP7cGOAPG+LrtBhay+5T5k22SG1MzaKe6rNKWwKOBh85ir&#39;)">Python爬虫框架Scrapy安装使用步骤</a><br></div></div>
<script async="async" src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/gpt.js"></script>
<script>
  var googletag = googletag || {};
  googletag.cmd = googletag.cmd || [];
</script>

<script>
  googletag.cmd.push(function() {
    googletag.defineSlot('/1090369/C2', [468, 60], 'div-gpt-ad-1539008685004-0').addService(googletag.pubads());
    googletag.pubads().enableSingleRequest();
    googletag.enableServices();
  });
</script>
<div id="cnblogs_c2" class="c_ad_block">
    <div id="div-gpt-ad-1539008685004-0" style="height:60px; width:468px;" data-google-query-id="CNbiqu_Ozd4CFVSPYgodfCMPhw">
    
    <div id="google_ads_iframe_/1090369/C2_0__container__" style="border: 0pt none;"><iframe id="google_ads_iframe_/1090369/C2_0" title="3rd party ad content" name="google_ads_iframe_/1090369/C2_0" width="468" height="60" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" style="border: 0px; vertical-align: bottom;" data-google-container-id="1" data-load-complete="true" src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/saved_resource.html"></iframe></div></div>
</div>
<div id="under_post_kb"><div class="itnews c_ad_block"><b>最新新闻</b>：<br> ·  <a href="https://news.cnblogs.com/n/611902/" target="_blank">双11刚过0点改不了收货地址 网友硬着头皮一通乱寄</a><br> ·  <a href="https://news.cnblogs.com/n/611900/" target="_blank">2000亿！天猫双11跨越里程碑</a><br> ·  <a href="https://news.cnblogs.com/n/611901/" target="_blank">英特尔39款至强新处理器曝光，最高28核205W</a><br> ·  <a href="https://news.cnblogs.com/n/611899/" target="_blank">再创新高！京东下单金额破1354亿 25岁以下用户大涨</a><br> ·  <a href="https://news.cnblogs.com/n/611898/" target="_blank">生物在量子进化？你可能正在利用量子现象生存</a><br>» <a href="http://news.cnblogs.com/" title="IT新闻" target="_blank">更多新闻...</a></div></div>
<div id="HistoryToday" class="c_ad_block"></div>
<script type="text/javascript">
    fixPostBody();
    setTimeout(function () { incrementViewCount(cb_entryId); }, 50);
    deliverAdT2();
    deliverAdC1();
    deliverAdC2();    
    loadNewsAndKb();
    loadBlogSignature();
    LoadPostInfoBlock(cb_blogId, cb_entryId, cb_blogApp, cb_blogUserGuid);
    GetPrevNextPost(cb_entryId, cb_blogId, cb_entryCreatedDate, cb_postType);
    loadOptUnderPost();
    GetHistoryToday(cb_blogId, cb_blogApp, cb_entryCreatedDate);   
</script>
</div>


</div>
</div>
<div id="leftmenu">


<h3>导航</h3>
<ul>
<li>
<a id="blog_nav_sitehome" href="https://www.cnblogs.com/">博客园</a></li>
<li>
<a id="blog_nav_myhome" class="two_words" href="https://www.cnblogs.com/mrchige/">首页</a></li>
<li>
<a id="blog_nav_newpost" rel="nofollow" href="https://i.cnblogs.com/EditPosts.aspx?opt=1">新随笔</a></li>
<li>
<a id="blog_nav_contact" accesskey="9" class="two_words" rel="nofollow" href="https://msg.cnblogs.com/send/%E5%90%83%E5%92%AF">联系</a></li>
<li>
<a id="blog_nav_rss" class="two_words" href="https://www.cnblogs.com/mrchige/rss">订阅</a>
<a id="blog_nav_rss_image" href="https://www.cnblogs.com/mrchige/rss"><img src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/xml.gif" alt="订阅"></a>
</li>
<li>
<a id="blog_nav_admin" class="two_words" rel="nofollow" href="https://i.cnblogs.com/">管理</a></li>
</ul>

<div id="blog-calendar" style=""><table id="blogCalendar" class="Cal" cellspacing="0" cellpadding="0" title="Calendar">
	<tbody><tr><td colspan="7"><table class="CalTitle" cellspacing="0">
		<tbody><tr><td class="CalNextPrev"><a href="javascript:void(0);" onclick="loadBlogCalendar(&#39;2018/10/01&#39;);return false;">&lt;</a></td><td align="center">2018年11月</td><td class="CalNextPrev" align="right"><a href="javascript:void(0);" onclick="loadBlogCalendar(&#39;2018/12/01&#39;);return false;">&gt;</a></td></tr>
	</tbody></table></td></tr><tr><th class="CalDayHeader" align="center" abbr="日" scope="col">日</th><th class="CalDayHeader" align="center" abbr="一" scope="col">一</th><th class="CalDayHeader" align="center" abbr="二" scope="col">二</th><th class="CalDayHeader" align="center" abbr="三" scope="col">三</th><th class="CalDayHeader" align="center" abbr="四" scope="col">四</th><th class="CalDayHeader" align="center" abbr="五" scope="col">五</th><th class="CalDayHeader" align="center" abbr="六" scope="col">六</th></tr><tr><td class="CalOtherMonthDay" align="center">28</td><td class="CalOtherMonthDay" align="center">29</td><td class="CalOtherMonthDay" align="center">30</td><td class="CalOtherMonthDay" align="center">31</td><td align="center">1</td><td align="center">2</td><td class="CalWeekendDay" align="center">3</td></tr><tr><td class="CalWeekendDay" align="center">4</td><td align="center">5</td><td align="center">6</td><td align="center">7</td><td align="center">8</td><td align="center">9</td><td class="CalWeekendDay" align="center">10</td></tr><tr><td class="CalWeekendDay" align="center">11</td><td class="CalTodayDay" align="center">12</td><td align="center">13</td><td align="center">14</td><td align="center">15</td><td align="center">16</td><td class="CalWeekendDay" align="center">17</td></tr><tr><td class="CalWeekendDay" align="center">18</td><td align="center">19</td><td align="center">20</td><td align="center">21</td><td align="center">22</td><td align="center">23</td><td class="CalWeekendDay" align="center">24</td></tr><tr><td class="CalWeekendDay" align="center">25</td><td align="center">26</td><td align="center">27</td><td align="center">28</td><td align="center">29</td><td align="center">30</td><td class="CalOtherMonthDay" align="center">1</td></tr><tr><td class="CalOtherMonthDay" align="center">2</td><td class="CalOtherMonthDay" align="center">3</td><td class="CalOtherMonthDay" align="center">4</td><td class="CalOtherMonthDay" align="center">5</td><td class="CalOtherMonthDay" align="center">6</td><td class="CalOtherMonthDay" align="center">7</td><td class="CalOtherMonthDay" align="center">8</td></tr>
</tbody></table></div><script type="text/javascript">loadBlogDefaultCalendar();</script>
<meta name="vs_showGrid" content="False">

<h3>公告</h3>
<div id="blog-news"><div id="profile_block">昵称：<a href="https://home.cnblogs.com/u/mrchige/">吃咯</a><br>园龄：<a href="https://home.cnblogs.com/u/mrchige/" title="入园时间：2016-09-25">2年1个月</a><br>粉丝：<a href="https://home.cnblogs.com/u/mrchige/followers/">60</a><br>关注：<a href="https://home.cnblogs.com/u/mrchige/followees/">0</a><div id="p_b_follow"><a href="javascript:void(0);" onclick="follow(&#39;10291468-bd82-e611-845c-ac853d9f53ac&#39;)">+加关注</a></div><script>getFollowStatus('10291468-bd82-e611-845c-ac853d9f53ac')</script></div></div><script type="text/javascript">loadBlogNews();</script>

<div id="blog-sidecolumn"><div id="sidebar_search" class="sidebar-block">
<div id="sidebar_search" class="mySearch">
<h3 class="catListTitle">搜索</h3>
<div id="sidebar_search_box">
<div id="widget_my_zzk" class="div_my_zzk"><input type="text" id="q" onkeydown="return zzk_go_enter(event);" class="input_my_zzk">&nbsp;<input onclick="zzk_go()" type="button" value="找找看" id="btnZzk" class="btn_my_zzk"></div>
<div id="widget_my_google" class="div_my_zzk"><input type="text" name="google_q" id="google_q" onkeydown="return google_go_enter(event)" class="input_my_zzk">&nbsp;<input onclick="google_go()" type="button" value="谷歌搜索" class="btn_my_zzk"></div>
</div>
</div>

</div><div id="sidebar_shortcut" class="sidebar-block">
<h3 class="catListTitle">常用链接</h3>
<ul>
<li><a href="https://www.cnblogs.com/mrchige/p/" title="我的博客的随笔列表">我的随笔</a></li><li><a href="https://www.cnblogs.com/mrchige/MyComments.html" title="我发表过的评论列表">我的评论</a></li><li><a href="https://www.cnblogs.com/mrchige/OtherPosts.html" title="我评论过的随笔列表">我的参与</a></li><li><a href="https://www.cnblogs.com/mrchige/RecentComments.html" title="我的博客的评论列表">最新评论</a></li><li><a href="https://www.cnblogs.com/mrchige/tag/" title="我的博客的标签列表">我的标签</a></li>
</ul>
<div id="itemListLin_con" style="display:none;">

</div></div><div id="sidebar_toptags" class="sidebar-block">
<h3 class="catListTitle">我的标签</h3>
<div id="MyTag">
<ul>
<li><a href="https://www.cnblogs.com/mrchige/tag/Python/">Python</a>(106)</li><li><a href="https://www.cnblogs.com/mrchige/tag/mysql/">mysql</a>(9)</li><li><a href="https://www.cnblogs.com/mrchige/tag/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>(6)</li><li><a href="https://www.cnblogs.com/mrchige/tag/amazon/">amazon</a>(6)</li><li><a href="https://www.cnblogs.com/mrchige/tag/aws/">aws</a>(6)</li><li><a href="https://www.cnblogs.com/mrchige/tag/git/">git</a>(5)</li><li><a href="https://www.cnblogs.com/mrchige/tag/LabVIEW/">LabVIEW</a>(5)</li><li><a href="https://www.cnblogs.com/mrchige/tag/storm/">storm</a>(5)</li><li><a href="https://www.cnblogs.com/mrchige/tag/%E8%B7%AF%E7%94%B1%E8%A1%A8/">路由表</a>(5)</li><li><a href="https://www.cnblogs.com/mrchige/tag/amazom_aws/">amazom_aws</a>(4)</li><li><a href="https://www.cnblogs.com/mrchige/tag/">更多</a></li>
</ul>
</div></div><div id="sidebar_categories">
		<h3>随笔档案</h3>
		
				<ul>
			
				<li><a id="CatList_LinkList_0_Link_0" href="https://www.cnblogs.com/mrchige/archive/2017/03.html">2017年3月 (13)</a></li>
			
				<li><a id="CatList_LinkList_0_Link_1" href="https://www.cnblogs.com/mrchige/archive/2017/02.html">2017年2月 (71)</a></li>
			
				<li><a id="CatList_LinkList_0_Link_2" href="https://www.cnblogs.com/mrchige/archive/2017/01.html">2017年1月 (15)</a></li>
			
				<li><a id="CatList_LinkList_0_Link_3" href="https://www.cnblogs.com/mrchige/archive/2016/12.html">2016年12月 (12)</a></li>
			
				<li><a id="CatList_LinkList_0_Link_4" href="https://www.cnblogs.com/mrchige/archive/2016/11.html">2016年11月 (1)</a></li>
			
				<li><a id="CatList_LinkList_0_Link_5" href="https://www.cnblogs.com/mrchige/archive/2016/10.html">2016年10月 (8)</a></li>
			
				<li><a id="CatList_LinkList_0_Link_6" href="https://www.cnblogs.com/mrchige/archive/2016/09.html">2016年9月 (30)</a></li>
			
				</ul>
			
	
		<h3>文章档案</h3>
		
				<ul>
			
				<li><a id="CatList_LinkList_1_Link_0" href="https://www.cnblogs.com/mrchige/archives/2016/10.html" rel="nofollow">2016年10月 (14)</a></li>
			
				<li><a id="CatList_LinkList_1_Link_1" href="https://www.cnblogs.com/mrchige/archives/2016/09.html" rel="nofollow">2016年9月 (4)</a></li>
			
				</ul>
			
	</div><div id="sidebar_recentcomments" class="sidebar-block"><div id="recent_comments_wrap">
<h3 class="catListTitle">最新评论</h3>
<div class="RecentComment" id="RecentComments">
	<div id="RecentCommentsBlock"><ul>
        <li class="recent_comment_title"><a href="https://www.cnblogs.com/mrchige/p/6434399.html#4032742">1. Re:Python进阶之“属性（property）”详解</a></li>
        <li class="recent_comment_body">讲解的很清楚</li>
        <li class="recent_comment_author">--数据分析玩家</li>
        <li class="recent_comment_title"><a href="https://www.cnblogs.com/mrchige/p/5999923.html#4009318">2. Re:算法第四版 coursera公开课 普林斯顿算法 ⅠⅡ部分 Robert Sedgewick主讲《Algorithms》</a></li>
        <li class="recent_comment_body">兄弟，怎么下载不了？</li>
        <li class="recent_comment_author">--e891377</li>
        <li class="recent_comment_title"><a href="https://www.cnblogs.com/mrchige/p/6425775.html#3975846">3. Re:python多线程</a></li>
        <li class="recent_comment_body">文章很好，谢谢分享</li>
        <li class="recent_comment_author">--gaby_yan</li>
        <li class="recent_comment_title"><a href="https://www.cnblogs.com/mrchige/p/6495147.html#3936040">4. Re:Scapy实现SYN泛洪攻击</a></li>
        <li class="recent_comment_body">你好，我用node起了一个localhost：8080的服务，然后用这个while：True无限发包，然后这个服务一直好好的，电脑也没任何反应，真的能做到syn flood攻击么。。。。。。。</li>
        <li class="recent_comment_author">--swallowblank</li>
        <li class="recent_comment_title"><a href="https://www.cnblogs.com/mrchige/p/6346601.html#3927533">5. Re:使用Spark MLlib进行情感分析</a></li>
        <li class="recent_comment_body">这个对中文支撑ok？</li>
        <li class="recent_comment_author">--csanycall</li>
</ul>
</div>
</div>
</div></div><div id="sidebar_topviewedposts" class="sidebar-block"><div id="topview_posts_wrap">
<h3 class="catListTitle">阅读排行榜</h3>
<div class="RecentComment" id="TopViewPosts"> 
	<div id="TopViewPostsBlock"><ul><li><a href="https://www.cnblogs.com/mrchige/p/6371783.html">1. python控制流 If-else(82462)</a></li><li><a href="https://www.cnblogs.com/mrchige/p/6389588.html">2. SQLAlchemy 教程 —— 基础入门篇(20667)</a></li><li><a href="https://www.cnblogs.com/mrchige/p/6379911.html">3. Python 图片转字符画(20280)</a></li><li><a href="https://www.cnblogs.com/mrchige/p/5920253.html">4. Linux下Apache服务的查看和启动(16892)</a></li><li><a href="https://www.cnblogs.com/mrchige/p/6387664.html">5. python的项目结构(15023)</a></li></ul></div>
</div>
</div></div><div id="sidebar_topcommentedposts" class="sidebar-block"><div id="topfeedback_posts_wrap">
<h3 class="catListTitle">评论排行榜</h3>
<div class="RecentComment" id="TopCommentsPosts">
	<div id="TopFeedbackPostsBlock"><ul><li><a href="https://www.cnblogs.com/mrchige/p/5999923.html">1. 算法第四版 coursera公开课 普林斯顿算法 ⅠⅡ部分 Robert Sedgewick主讲《Algorithms》(2)</a></li><li><a href="https://www.cnblogs.com/mrchige/p/6389588.html">2. SQLAlchemy 教程 —— 基础入门篇(1)</a></li><li><a href="https://www.cnblogs.com/mrchige/p/6346601.html">3. 使用Spark MLlib进行情感分析(1)</a></li><li><a href="https://www.cnblogs.com/mrchige/p/6495147.html">4. Scapy实现SYN泛洪攻击(1)</a></li><li><a href="https://www.cnblogs.com/mrchige/p/6434399.html">5. Python进阶之“属性（property）”详解(1)</a></li></ul></div>
</div></div></div><div id="sidebar_topdiggedposts" class="sidebar-block"><div id="topdigg_posts_wrap">
<h3 class="catListTitle">推荐排行榜</h3>
<div class="RecentComment">
	<div id="TopDiggPostsBlock"><ul><li><a href="https://www.cnblogs.com/mrchige/p/6389588.html">1. SQLAlchemy 教程 —— 基础入门篇(6)</a></li><li><a href="https://www.cnblogs.com/mrchige/p/5920253.html">2. Linux下Apache服务的查看和启动(2)</a></li><li><a href="https://www.cnblogs.com/mrchige/p/6177905.html">3. python的dir、help、str用法(1)</a></li><li><a href="https://www.cnblogs.com/mrchige/p/5999923.html">4. 算法第四版 coursera公开课 普林斯顿算法 ⅠⅡ部分 Robert Sedgewick主讲《Algorithms》(1)</a></li><li><a href="https://www.cnblogs.com/mrchige/p/5927200.html">5. win7下，使用django运行django-admin.py无法创建网站(1)</a></li></ul></div>
</div></div></div></div><script type="text/javascript">loadBlogSideColumn();</script>

</div>
</div>
<div class="clear"></div>
<div id="footer">

<p id="footer">
	Powered by: 
	<br>
	
	<a id="Footer1_Hyperlink3" name="Hyperlink1" href="https://www.cnblogs.com/" style="font-family:Verdana;font-size:12px;">博客园</a>
	<br>
	Copyright © 吃咯
</p>
</div>
</div>



<iframe id="google_osd_static_frame_6073772033946" name="google_osd_static_frame" style="display: none; width: 0px; height: 0px;" src="./基于scrapy爬虫的天气数据采集(python) - 吃咯 - 博客园_files/saved_resource(1).html"></iframe></body></html>